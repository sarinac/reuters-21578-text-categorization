{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Labeling and Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from modules.preprocessing.tokenizer import body_to_token\n",
    "from modules.preprocessing.vectorizer import generate_bow, vectorize_data\n",
    "from modules.utils.s3 import get_from_s3, put_to_s3\n",
    "from constants import BUCKET, DATA_CLEAN_PREFIX, DATA_RAW_PREFIX, DATA_PROCESSED_PREFIX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Y Labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `TOPICS` tags in the Reuters corpus contain topic classifiers for the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved 135 labels.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'acq, alum, austdlr, austral, barley, bfr, bop, can, carcass, castor-meal, castor-oil, castorseed, citruspulp, cocoa, coconut, coconut-oil, coffee, copper, copra-cake, corn, corn-oil, cornglutenfeed, cotton, cotton-meal, cotton-oil, cottonseed, cpi, cpu, crude, cruzado, dfl, dkr, dlr, dmk, drachma, earn, escudo, f-cattle, ffr, fishmeal, flaxseed, fuel, gas, gnp, gold, grain, groundnut, groundnut-meal, groundnut-oil, heat, hk, hog, housing, income, instal-debt, interest, inventories, ipi, iron-steel, jet, jobs, l-cattle, lead, lei, lin-meal, lin-oil, linseed, lit, livestock, lumber, lupin, meal-feed, mexpeso, money-fx, money-supply, naphtha, nat-gas, nickel, nkr, nzdlr, oat, oilseed, orange, palladium, palm-meal, palm-oil, palmkernel, peseta, pet-chem, platinum, plywood, pork-belly, potato, propane, rand, rape-meal, rape-oil, rapeseed, red-bean, reserves, retail, rice, ringgit, rubber, rupiah, rye, saudriyal, sfr, ship, silk, silver, singdlr, skr, sorghum, soy-meal, soy-oil, soybean, stg, strategic-metal, sugar, sun-meal, sun-oil, sunseed, tapioca, tea, tin, trade, tung, tung-oil, veg-oil, wheat, wool, wpi, yen, zinc'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get all possible labels\n",
    "labels = get_from_s3(BUCKET, f\"{DATA_RAW_PREFIX}all-topics-strings.lc.txt\")\n",
    "labels = list(map(lambda x: x.strip(), labels.strip().split(\"\\n\")))\n",
    "print(f\"Retrieved {len(labels)} labels.\")\n",
    "\", \".join(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Word Tokens and Split Train and Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading entry #1000...\n"
     ]
    }
   ],
   "source": [
    "# Get Y labels, X words, and train/test split\n",
    "\n",
    "# Create data container\n",
    "template = {\"X\": [], \"Y\": [], \"ids\": []}\n",
    "data = {\"train\": copy.deepcopy(template), \"test\": copy.deepcopy(template)}\n",
    "\n",
    "# Get entries from S3\n",
    "response = get_from_s3(BUCKET, f\"{DATA_CLEAN_PREFIX}dataset.json\")\n",
    "all_entries = json.loads(response)\n",
    "\n",
    "for index, entry in enumerate(all_entries, 1):\n",
    "    if index % 1000 == 0:\n",
    "        print(\"Reading entry #{}...\".format(index))\n",
    "\n",
    "    # Drop record if not used in train/test dataset\n",
    "    if entry[\"split\"] == \"not-used\" or entry[\"body\"] == \"\":\n",
    "        continue\n",
    "\n",
    "    # Tokenize text body (Note: takes a while to run)\n",
    "    token = body_to_token(entry[\"body\"])\n",
    "    \n",
    "    # Drop record if there is no text body after vectorization\n",
    "    if len(token) == 0:\n",
    "        continue\n",
    "    \n",
    "    # Get training labels\n",
    "    y_labels = [int(label.strip() in entry[\"topics\"]) for label in labels]\n",
    "    \n",
    "    # Add to data\n",
    "    train_or_test = entry[\"split\"]\n",
    "    data[train_or_test][\"X\"].append(token)\n",
    "    data[train_or_test][\"Y\"].append(y_labels)\n",
    "    data[train_or_test][\"ids\"].append(entry[\"id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to file locally\n",
    "\n",
    "with open(\"data/pretrain_X.json\", \"w\") as file:\n",
    "    json.dump(data[\"train\"][\"X\"], file)\n",
    "with open(\"data/pretest_X.json\", \"w\") as file:\n",
    "    json.dump(data[\"test\"][\"X\"], file)\n",
    "\n",
    "with open(\"data/pretrain_Y.json\", \"w\") as file:\n",
    "    json.dump(data[\"train\"][\"Y\"], file)\n",
    "with open(\"data/pretest_Y.json\", \"w\") as file:\n",
    "    json.dump(data[\"test\"][\"Y\"], file)\n",
    "\n",
    "with open(\"data/pretrain_ids.json\", \"w\") as file:\n",
    "    json.dump(data[\"train\"][\"ids\"], file)\n",
    "with open(\"data/pretest_ids.json\", \"w\") as file:\n",
    "    json.dump(data[\"test\"][\"ids\"], file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload to S3\n",
    "\n",
    "put_to_s3(json.dumps(data[\"train\"][\"X\"]), BUCKET, f\"{DATA_PROCESSED_PREFIX}vectorized/train_X.json\")\n",
    "put_to_s3(json.dumps(data[\"train\"][\"Y\"]), BUCKET, f\"{DATA_PROCESSED_PREFIX}vectorized/train_Y.json\")\n",
    "put_to_s3(json.dumps(data[\"train\"][\"ids\"]), BUCKET, f\"{DATA_PROCESSED_PREFIX}vectorized/train_ids.json\")\n",
    "\n",
    "put_to_s3(json.dumps(data[\"test\"][\"X\"]), BUCKET, f\"{DATA_PROCESSED_PREFIX}vectorized/test_X.json\")\n",
    "put_to_s3(json.dumps(data[\"test\"][\"Y\"]), BUCKET, f\"{DATA_PROCESSED_PREFIX}vectorized/test_Y.json\")\n",
    "put_to_s3(json.dumps(data[\"test\"][\"ids\"]), BUCKET, f\"{DATA_PROCESSED_PREFIX}vectorized/test_ids.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Bag of Words to Create Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "train_X = json.loads(get_from_s3(BUCKET, f\"{DATA_PROCESSED_PREFIX}vectorized/train_X.json\"))\n",
    "train_Y = json.loads(get_from_s3(BUCKET, f\"{DATA_PROCESSED_PREFIX}vectorized/train_Y.json\"))\n",
    "\n",
    "test_X = json.loads(get_from_s3(BUCKET, f\"{DATA_PROCESSED_PREFIX}vectorized/test_X.json\"))\n",
    "test_Y = json.loads(get_from_s3(BUCKET, f\"{DATA_PROCESSED_PREFIX}vectorized/test_Y.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_counter = generate_bow(train_X)\n",
    "lengths = list(map(lambda x: len(x), train_X))\n",
    "print(f\"Out of {len(train_X)} documents, the shortest has {min(lengths)} words and the longest has {max(lengths)} words.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create indexer to encode words\n",
    "word_encoding = {word: index for index, word in enumerate(freq_counter.keys(), 2)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply on both train and test dataset\n",
    "train_X = vectorize_data(word_encoding, train_X)\n",
    "test_X = vectorize_data(word_encoding, test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all to DataFrame\n",
    "train_X = pd.DataFrame(train_X)\n",
    "train_Y = pd.DataFrame(train_Y)\n",
    "test_X = pd.DataFrame(test_X)\n",
    "test_Y = pd.DataFrame(test_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save as csv\n",
    "train_X.to_csv(\"data/train_X.csv\", index=False, header=False)\n",
    "train_Y.to_csv(\"data/train_Y.csv\", index=False, header=False)\n",
    "test_X.to_csv(\"data/test_X.csv\", index=False, header=False)\n",
    "test_Y.to_csv(\"data/test_Y.csv\", index=False, header=False)\n",
    "\n",
    "put_to_s3(train_X.to_csv(index=False, header=False), BUCKET, f\"{DATA_PROCESSED_PREFIX}train_X.csv\")\n",
    "put_to_s3(train_Y.to_csv(index=False, header=False), BUCKET, f\"{DATA_PROCESSED_PREFIX}train_Y.csv\")\n",
    "put_to_s3(test_X.to_csv(index=False, header=False), BUCKET, f\"{DATA_PROCESSED_PREFIX}test_X.csv\")\n",
    "put_to_s3(test_Y.to_csv(index=False, header=False), BUCKET, f\"{DATA_PROCESSED_PREFIX}test_Y.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-west-1:742091327244:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
