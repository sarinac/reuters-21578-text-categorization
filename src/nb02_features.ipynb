{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Labeling and Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import boto3\n",
    "\n",
    "from constants import BUCKET, DATA_CLEAN_PREFIX, DATA_RAW_PREFIX, DATA_PROCESSED_PREFIX\n",
    "\n",
    "\n",
    "# Create s3 client\n",
    "s3 = boto3.client(\"s3\",\n",
    "    region_name=\"us-west-1\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Y Labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `TOPICS` tags in the Reuters corpus contain topic classifiers for the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved 135 labels.\n"
     ]
    }
   ],
   "source": [
    "# Get all possible labels\n",
    "\n",
    "s3_object = s3.get_object(Bucket=BUCKET, Key=f\"{DATA_RAW_PREFIX}all-topics-strings.lc.txt\")\n",
    "if s3_object[\"ResponseMetadata\"][\"HTTPStatusCode\"] == 200:\n",
    "    response = s3_object[\"Body\"].read()\n",
    "    labels = response.decode().strip().split(\"\\n\")\n",
    "    print(f\"Retrieved {len(labels)} labels.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'acq, alum, austdlr, austral, barley, bfr, bop, can, carcass, castor-meal, castor-oil, castorseed, citruspulp, cocoa, coconut, coconut-oil, coffee, copper, copra-cake, corn, corn-oil, cornglutenfeed, cotton , cotton-meal, cotton-oil, cottonseed, cpi, cpu, crude, cruzado, dfl, dkr, dlr, dmk, drachma, earn, escudo, f-cattle, ffr, fishmeal, flaxseed, fuel, gas, gnp, gold, grain, groundnut, groundnut-meal, groundnut-oil, heat, hk, hog, housing, income, instal-debt, interest, inventories, ipi, iron-steel, jet, jobs, l-cattle, lead, lei, lin-meal, lin-oil, linseed, lit, livestock, lumber, lupin, meal-feed, mexpeso, money-fx, money-supply, naphtha     , nat-gas, nickel, nkr, nzdlr, oat, oilseed, orange, palladium, palm-meal, palm-oil, palmkernel, peseta, pet-chem, platinum, plywood, pork-belly, potato, propane, rand, rape-meal, rape-oil, rapeseed, red-bean, reserves, retail, rice, ringgit, rubber, rupiah, rye, saudriyal, sfr, ship, silk, silver, singdlr, skr, sorghum, soy-meal, soy-oil, soybean, stg, strategic-metal, sugar, sun-meal, sun-oil, sunseed, tapioca, tea, tin, trade, tung, tung-oil, veg-oil, wheat, wool, wpi, yen, zinc'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\", \".join(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Word Tokens and Split Train and Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from modules.preprocessing.clean import body_to_token, generate_bow, vectorize_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get sgm filenames\n",
    "\n",
    "s3_objects = s3.list_objects(\n",
    "    Bucket=BUCKET,\n",
    "    Prefix=DATA_CLEAN_PREFIX,\n",
    ")[\"Contents\"]\n",
    "s3_objects = list(map(lambda x: x[\"Key\"], s3_objects))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Y labels, X words, and train/test split\n",
    "\n",
    "Y = []\n",
    "X = []\n",
    "is_train = []\n",
    "is_test = []\n",
    "\n",
    "\n",
    "s3_object = s3.get_object(Bucket=BUCKET, Key=f\"{DATA_CLEAN_PREFIX}dataset.json\")\n",
    "if s3_object[\"ResponseMetadata\"][\"HTTPStatusCode\"] == 200:\n",
    "    response = s3_object[\"Body\"].read()\n",
    "    all_entries = json.loads(response.decode())  # list of entries\n",
    "\n",
    "    for entry in all_entries:\n",
    "        \n",
    "        if entry[\"split\"] == \"not-used\":\n",
    "            next\n",
    "\n",
    "        # Tokenize text body (Note: takes a while to run)\n",
    "        X.append(body_to_token(entry[\"body\"]))\n",
    "\n",
    "        # Label Y\n",
    "        Y.append([int(label in entry[\"topics\"]) for label in labels])\n",
    "\n",
    "        # Determine how to split data: train, test, or not-used\n",
    "        is_train += [1 if entry[\"split\"] == \"train\" else 0]\n",
    "        is_test += [1 if entry[\"split\"] == \"test\" else 0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split to train and test data\n",
    "\n",
    "np_X = np.array(X, dtype=object)\n",
    "np_Y = np.array(Y, dtype=object)\n",
    "\n",
    "\n",
    "tbl = np.vstack((is_train, np_X))  # shape: 2, 21578\n",
    "train_X = tbl[1, tbl[0,:] == 1]  # shape: 14668, 1\n",
    "\n",
    "tbl = np.vstack((is_test, np_X))  # shape: 2, 21578\n",
    "test_X = tbl[1, tbl[0,:] == 1]  # shape: 6188, 1\n",
    "\n",
    "tbl = np.vstack((is_train, np_Y.T))  # shape: 136, 21578\n",
    "train_Y = tbl[1:, tbl[0,:] == 1].T  # shape: 14668, 135\n",
    "\n",
    "tbl = np.vstack((is_test, np_Y.T))  # shape: 136, 21578\n",
    "test_Y = tbl[1:, tbl[0,:] == 1].T  # shape: 6188, 135"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to file\n",
    "\n",
    "with open(\"data/pretrain_X.txt\", \"w\") as file:\n",
    "    json.dump(train_X.tolist(), file)\n",
    "with open(\"data/pretest_X.txt\", \"w\") as file:\n",
    "    json.dump(test_X.tolist(), file)\n",
    "with open(\"data/pretrain_Y.txt\", \"w\") as file:\n",
    "    json.dump(train_Y.tolist(), file)\n",
    "with open(\"data/pretest_Y.txt\", \"w\") as file:\n",
    "    json.dump(test_Y.tolist(), file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open file\n",
    "\n",
    "with open(\"data/pretrain_X.txt\", \"r\") as file:\n",
    "    train_X = json.load(file)\n",
    "with open(\"data/pretest_X.txt\", \"r\") as file:\n",
    "    test_X = json.load(file)\n",
    "with open(\"data/pretrain_Y.txt\", \"r\") as file:\n",
    "    train_Y = json.load(file)\n",
    "with open(\"data/pretest_Y.txt\", \"r\") as file:\n",
    "    test_Y = json.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore Word Similarity\n",
    "\n",
    "Try: Use `nn.Embedding` in PyTorch instead of Word2Vec."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gensim\n",
    "\n",
    "# # Create continuous bag of words based on training data\n",
    "# model = gensim.models.Word2Vec(train_X, min_count=1)\n",
    "\n",
    "# print(f\"Similarity of 'corn' and 'crop' is {model.wv.similarity(\"corn\", \"crop\")}.\")\n",
    "\n",
    "# print(\"Most similar words...\")\n",
    "# model.wv.most_similar(\"share\")[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Bag of Words to Create Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        The most common words from 23196 documents are \n",
      "        said (35764), mln (17106), dlr (15886), reuter (13466), pct (12022).\n",
      "    \n",
      "Truncated word dictionary to 10000 words.\n",
      "Out of 14668 documents, the shortest has 0 words and the longest has 820 words.\n"
     ]
    }
   ],
   "source": [
    "freq_counter = generate_bow(train_X)\n",
    "lengths = list(map(lambda x: len(x), train_X))\n",
    "print(f\"Out of {len(train_X)} documents, the shortest has {min(lengths)} words and the longest has {max(lengths)} words.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create encoding\n",
    "word_encoding = {word: index for index, word in enumerate(freq_counter.keys(), 2)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply on both train and test dataset\n",
    "train_X = vectorize_data(word_encoding, train_X)\n",
    "test_X = vectorize_data(word_encoding, test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all to DataFrame\n",
    "train_X = pd.DataFrame(train_X)\n",
    "train_Y = pd.DataFrame(train_Y)\n",
    "test_X = pd.DataFrame(test_X)\n",
    "test_Y = pd.DataFrame(test_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out training data where there are no words\n",
    "no_0_words = np.array(train_X.sum(axis=1) > 0)\n",
    "train_X = train_X[no_0_words]\n",
    "train_Y = train_Y[no_0_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save as csv\n",
    "train_X.to_csv(\"data/train_X.csv\", index=False, header=False)\n",
    "train_Y.to_csv(\"data/train_Y.csv\", index=False, header=False)\n",
    "test_X.to_csv(\"data/test_X.csv\", index=False, header=False)\n",
    "test_Y.to_csv(\"data/test_Y.csv\", index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload to S3\n",
    "s3 = boto3.resource('s3')\n",
    "s3.meta.client.upload_file('./data/train_X.csv', BUCKET, f\"{DATA_PROCESSED_PREFIX}train_X.csv\")\n",
    "s3.meta.client.upload_file('./data/train_Y.csv', BUCKET, f\"{DATA_PROCESSED_PREFIX}train_Y.csv\")\n",
    "s3.meta.client.upload_file('./data/test_X.csv', BUCKET, f\"{DATA_PROCESSED_PREFIX}test_X.csv\")\n",
    "s3.meta.client.upload_file('./data/test_Y.csv', BUCKET, f\"{DATA_PROCESSED_PREFIX}test_Y.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-west-1:742091327244:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
